# -*- coding: utf-8 -*-
"""fairness_audit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oH7-GR9yEJv_Zij4pMccaKv31nLf3OiC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set style for professional charts
plt.style.use('ggplot')

def generate_mock_compas_data(n=1000):
    """
    Generates a synthetic dataset mimicking the statistical properties
    of the COMPAS dataset for demonstration purposes.

    In a real scenario, you would load this:
    df = pd.read_csv('compas-scores-two-years.csv')
    """
    np.random.seed(42)

    # 1. Create Race Distribution (roughly similar to COMPAS)
    races = ['African-American'] * 600 + ['Caucasian'] * 400
    np.random.shuffle(races)

    df = pd.DataFrame({'race': races})

    # 2. Simulate Ground Truth (Did they actually recidivate?)
    # Historical data often shows systemic over-policing, affecting these rates
    # We simulate a base rate, but slightly higher for AA due to systemic issues in data
    df['two_year_recid'] = 0

    for idx, row in df.iterrows():
        if row['race'] == 'African-American':
            # Slightly higher recidivism in data (reflecting systemic bias in arrests)
            df.at[idx, 'two_year_recid'] = np.random.choice([0, 1], p=[0.48, 0.52])
        else:
            df.at[idx, 'two_year_recid'] = np.random.choice([0, 1], p=[0.61, 0.39])

    # 3. Simulate Algorithm Risk Scores (The Biased AI)
    # The bias: The algorithm assigns higher scores to AA defendants
    # even when they don't recidivate (False Positives)
    df['decile_score'] = 0

    for idx, row in df.iterrows():
        base_score = np.random.randint(1, 11)

        # Injecting the bias found in ProPublica's analysis:
        # If African-American, skew scores higher regardless of actual recidivism
        if row['race'] == 'African-American':
            score = base_score + np.random.randint(0, 3)
        else:
            score = base_score - np.random.randint(0, 2)

        df.at[idx, 'decile_score'] = max(1, min(10, score))

    # Binary prediction: High Risk if score > 4 (Common threshold)
    df['predicted_high_risk'] = (df['decile_score'] > 4).astype(int)

    return df

def audit_fairness(df):
    print("--- STARTING FAIRNESS AUDIT ---\n")

    groups = ['African-American', 'Caucasian']
    metrics = {}

    for group in groups:
        subset = df[df['race'] == group]

        # Confusion Matrix Components
        # TP: Predicted High Risk AND Did Recidivate
        TP = len(subset[(subset['predicted_high_risk'] == 1) & (subset['two_year_recid'] == 1)])
        # TN: Predicted Low Risk AND Did NOT Recidivate
        TN = len(subset[(subset['predicted_high_risk'] == 0) & (subset['two_year_recid'] == 0)])
        # FP: Predicted High Risk BUT Did NOT Recidivate (The Innocents labeled risky)
        FP = len(subset[(subset['predicted_high_risk'] == 1) & (subset['two_year_recid'] == 0)])
        # FN: Predicted Low Risk BUT Did Recidivate
        FN = len(subset[(subset['predicted_high_risk'] == 0) & (subset['two_year_recid'] == 1)])

        total_negatives = TN + FP
        total_positives = TP + FN
        total_population = len(subset)

        # Metric 1: False Positive Rate (FPR)
        # % of innocent people wrongly labeled as high risk
        fpr = FP / total_negatives if total_negatives > 0 else 0

        # Metric 2: Selection Rate (for Disparate Impact)
        selection_rate = (TP + FP) / total_population

        metrics[group] = {
            'FPR': fpr,
            'Selection_Rate': selection_rate,
            'Count': total_population
        }

        print(f"Stats for {group}:")
        print(f"  Total: {total_population}")
        print(f"  False Positives (Innocents labeled risky): {FP}")
        print(f"  False Positive Rate: {fpr:.2%}")
        print("-" * 30)

    # Calculate Disparate Impact
    # Ratio of selection rate of unprivileged (AA) / privileged (Cauc)
    # Ideally should be between 0.8 and 1.25
    aa_sel = metrics['African-American']['Selection_Rate']
    c_sel = metrics['Caucasian']['Selection_Rate']

    disparate_impact = aa_sel / c_sel

    print(f"\n--- FAIRNESS METRICS ---")
    print(f"Disparate Impact Ratio: {disparate_impact:.2f}")
    if disparate_impact < 0.8:
        print("  -> RESULT: Bias Detected (Adverse Impact on African-Americans)")
    elif disparate_impact > 1.25:
         print("  -> RESULT: Bias Detected (Adverse Impact on Caucasians)")
    else:
        print("  -> RESULT: Within Fair Bounds")

    print(f"FPR Difference: {metrics['African-American']['FPR'] - metrics['Caucasian']['FPR']:.2%}")

    return metrics

def plot_results(metrics):
    groups = list(metrics.keys())
    fpr_values = [metrics[g]['FPR'] for g in groups]

    plt.figure(figsize=(10, 6))

    # Create colors
    colors = ['#d9534f', '#5bc0de'] # Red for AA (usually higher risk in this dataset), Blue for Cauc

    bars = plt.bar(groups, fpr_values, color=colors, alpha=0.8)

    plt.title('Audit of False Positive Rates by Race', fontsize=16)
    plt.ylabel('False Positive Rate (Innocents mislabeled)', fontsize=12)
    plt.ylim(0, 1.0)

    # Add labels on top of bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                 f'{height:.1%}',
                 ha='center', va='bottom', fontsize=14, fontweight='bold')

    plt.axhline(y=fpr_values[1], color='gray', linestyle='--', alpha=0.5, label='Privileged Group Baseline')
    plt.legend()

    # Add context text
    plt.figtext(0.5, -0.05,
                "Note: A higher False Positive Rate indicates the model is more likely to \nwrongly accuse innocent members of that group.",
                ha="center", fontsize=10, style='italic')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    # 1. Load/Create Data
    df = generate_mock_compas_data()

    # 2. Run Audit
    results = audit_fairness(df)

    # 3. Visualize
    plot_results(results)